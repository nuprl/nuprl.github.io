<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>PRL Blog: Posts tagged 'by Ming-Ho Yee'</title>
  <description>PRL Blog: Posts tagged 'by Ming-Ho Yee'</description>
  <link>http://prl.ccs.neu.edu/blog/tags/by-Ming-Ho-Yee.html</link>
  <lastBuildDate>Thu, 05 Sep 2019 10:00:00 UT</lastBuildDate>
  <pubDate>Thu, 05 Sep 2019 10:00:00 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>Lexical and Dynamic Scope</title>
   <link>http://prl.ccs.neu.edu/blog/2019/09/05/lexical-and-dynamic-scope/?utm_source=by-Ming-Ho-Yee&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2019-09-05-lexical-and-dynamic-scope</guid>
   <pubDate>Thu, 05 Sep 2019 10:00:00 UT</pubDate>
   <author>PRL</author>
   <description>
&lt;p&gt;This all started with a simple question about the R programming language: &lt;em&gt;is R lexically or dynamically scoped?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To answer that question, we need to understand what &lt;em&gt;scope&lt;/em&gt; is, along with &lt;em&gt;lexical scope&lt;/em&gt; and &lt;em&gt;dynamic scope&lt;/em&gt;.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;In this blog post, I&amp;rsquo;d like to explain the differences between lexical scope and dynamic scope, and also explore some of the history behind those ideas. In a subsequent post, I&amp;rsquo;ll discuss scoping in R and why it can be confusing.&lt;/p&gt;

&lt;h2 id="what-is-scope"&gt;What is scope?&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Scope&lt;/em&gt; refers to the places in a program where a variable is visible and can be referenced.&lt;/p&gt;

&lt;p&gt;An interesting situation is when a function has free variables. Consider the example below:&lt;/p&gt;

&lt;div class="brush: r"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt;1
2
3
4
5
6
7&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;x &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
f &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;a&lt;span class="p"&gt;)&lt;/span&gt; x &lt;span class="o"&gt;+&lt;/span&gt; a
g &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  x &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
  f&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
g&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# what does this return?&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;On line 1, we create a mapping for &lt;code&gt;x&lt;/code&gt; with value &lt;code&gt;1&lt;/code&gt;. On line 2, we define a function &lt;code&gt;f&lt;/code&gt; whose body uses the parameter &lt;code&gt;a&lt;/code&gt;, but also the free variable &lt;code&gt;x&lt;/code&gt;. On line 3, we define a function &lt;code&gt;g&lt;/code&gt;, whose body creates a new mapping for &lt;code&gt;x&lt;/code&gt; with value &lt;code&gt;2&lt;/code&gt;, and then calls &lt;code&gt;f(0)&lt;/code&gt;. (Note that line 4 does not update the mapping created on line 1.) Finally, on line 7, we call &lt;code&gt;g()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;What value does &lt;code&gt;g&lt;/code&gt; return when it is called? What mapping does the free variable &lt;code&gt;x&lt;/code&gt; on line 2 refer to? Does it refer to the mapping on line 1 that was visible when &lt;code&gt;f&lt;/code&gt; was defined? Or does it refer to the mapping on line 4 that was created just before &lt;code&gt;f&lt;/code&gt; was called?&lt;/p&gt;

&lt;h3 id="lexical-scoping"&gt;Lexical scoping&lt;/h3&gt;

&lt;p&gt;Under &lt;em&gt;lexical scoping&lt;/em&gt; (also known as &lt;em&gt;static scoping&lt;/em&gt;), the scope of a variable is determined by the lexical (&lt;em&gt;i.e.&lt;/em&gt;, textual) structure of a program.&lt;/p&gt;

&lt;p&gt;In the example above, the definition of &lt;code&gt;x&lt;/code&gt; on line 1 creates a scope that starts after its definition and extends &lt;em&gt;into&lt;/em&gt; the bodies of &lt;code&gt;f&lt;/code&gt; and &lt;code&gt;g&lt;/code&gt;. However, the second definition of &lt;code&gt;x&lt;/code&gt; on line 4 creates a new scope that (1) shadows the previous definition of &lt;code&gt;x&lt;/code&gt;, and (2) does not extend into the call &lt;code&gt;f(0)&lt;/code&gt; on line 5. Looking at this from another direction, the use of &lt;code&gt;x&lt;/code&gt; on line 2 is within the scope created by the definition on line 1, and thus refers to that definition.&lt;/p&gt;

&lt;p&gt;Therefore, under lexical scoping, the example program returns &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Most programming languages we use today are lexically scoped. Intuitively, a human (or compiler) can determine the scope of a variable by just examining the source code of a program. In other words, a compiler can determine which &lt;em&gt;definition&lt;/em&gt; each variable refers to&amp;mdash;but it may not be able to determine the &lt;em&gt;values&lt;/em&gt; of each variable.&lt;/p&gt;

&lt;h3 id="dynamic-scoping"&gt;Dynamic scoping&lt;/h3&gt;

&lt;p&gt;Under &lt;em&gt;dynamic scoping&lt;/em&gt;, a variable is bound to the most recent value assigned to that variable, &lt;em&gt;i.e.&lt;/em&gt;, the most recent assignment &lt;em&gt;during the program&amp;rsquo;s execution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In the example above, the free variable &lt;code&gt;x&lt;/code&gt; in the body of &lt;code&gt;f&lt;/code&gt; is evaluated when &lt;code&gt;f(0)&lt;/code&gt; is called on line 5. At that point (during program execution), the most recent assignment was on line 4.&lt;/p&gt;

&lt;p&gt;Therefore, under dynamic scoping, the example program returns &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Dynamically scoped programming languages include bash, LaTeX, and the original version of Lisp. Emacs Lisp is dynamically scoped, but allows the programmer to select lexical scoping. Conversely, Perl and Common Lisp are lexically scoped by default, but allow the programmer to select dynamic scoping.&lt;/p&gt;

&lt;h2 id="now-for-a-digression"&gt;Now for a digression&lt;/h2&gt;

&lt;p&gt;These are the definitions I learned from my classes and textbooks, and should be similar to other definitions and explanations you might find online.&lt;/p&gt;

&lt;p&gt;However, it took me many drafts and attempts before arriving at the current version. I had difficulty writing an explanation that I was satisfied with&amp;mdash;a definition that was not circular, did not appeal to some intuition or familiarity, and did not conflate terms. Even some of the resources I consulted had these issues.&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-1-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-1-return"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I am much happier with my current version, but it still bothers me slightly. If lexical scope and dynamic scope are related concepts, then why are the definitions so different? Why does the definition for &lt;em&gt;dynamic scope&lt;/em&gt; not mention scope at all? If &lt;em&gt;scope&lt;/em&gt; is about &amp;ldquo;where a variable is visible,&amp;rdquo; and that definition is with respect to a &lt;em&gt;variable definition&lt;/em&gt;, then why do so many explanations and examples define lexical and dynamic scope in terms of &lt;em&gt;variable use&lt;/em&gt;?&lt;/p&gt;

&lt;h2 id="scope-and-extent"&gt;Scope and Extent&lt;/h2&gt;

&lt;p&gt;I found some answers in Guy Steele&amp;rsquo;s &lt;em&gt;Common Lisp the Language, 2nd Edition&lt;/em&gt;,&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-2-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-2-return"&gt;2&lt;/a&gt;&lt;/sup&gt; which Matthias Felleisen recommended to me.&lt;/p&gt;

&lt;p&gt;In chapter 3, Steele introduces the concepts of &lt;em&gt;scope&lt;/em&gt; and &lt;em&gt;extent&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;&lt;em&gt;Scope&lt;/em&gt; refers to the spatial or textual region of the program within which references may occur. &lt;em&gt;Extent&lt;/em&gt; refers to the interval of time during which references may occur.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In addition, there are four interesting cases of scope and extent, with respect to Common Lisp:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;em&gt;Lexical scope&lt;/em&gt;: a reference can only occur within certain textual regions  of the program, which are determined by the establishing construct, &lt;em&gt;e.g.&lt;/em&gt;,  the body of a variable definition.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;em&gt;Indefinite scope&lt;/em&gt;: a reference can occur anywhere in the program.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;em&gt;Dynamic extent&lt;/em&gt;: a reference can occur during the time between an entity&amp;rsquo;s  creation and its explicit destruction, &lt;em&gt;e.g.&lt;/em&gt;, when a local variable is  created upon entering a function and destroyed when returning from that  function.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;em&gt;Indefinite extent&lt;/em&gt;: an entity may exist as long as it is possible to be  referenced. (Note that this is the idea behind garbage collection: an  entity can be destroyed once references to it are impossible.)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Steele points out that &lt;em&gt;dynamic scope&lt;/em&gt; is a misnomer, even though it is both a traditional and useful concept. It can be defined as &lt;em&gt;indefinite scope and dynamic extent&lt;/em&gt;. In other words, references to a variable may occur anywhere in a program, as long as that variable has been initialized and has not yet been explicitly destroyed. Furthermore, a later initialization hides an earlier one.&lt;/p&gt;

&lt;h3 id="discussion"&gt;Discussion&lt;/h3&gt;

&lt;p&gt;I found this approach very informative, because it explicitly distinguishes between space (scope) and time (extent), which further implies a separation between compile time and run time. This explains my unease with the definition of &amp;ldquo;dynamic scope&amp;rdquo;&amp;mdash;it is nominally about textual regions in a program, but also requires consideration of run-time behaviour. Dynamic scope is a misnomer!&lt;/p&gt;

&lt;p&gt;The above definitions are specifically for Common Lisp, but I believe we can learn from them and adapt them for other programming languages.&lt;/p&gt;

&lt;h2 id="a-brief-and-incomplete-history-of-lexical-scope"&gt;A brief and incomplete history of lexical scope&lt;/h2&gt;

&lt;p&gt;During my research of different definitions of lexical scope, I began to wonder if there was an &amp;ldquo;original&amp;rdquo; definition of lexical scope. I did not find one, but I was able to trace some of the connections between Lisp, Scheme, and ALGOL 60. This history is certainly incomplete, but I hope it is somewhat useful and interesting.&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;1960&lt;/strong&gt;. John McCarthy publishes the original paper on Lisp.&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-3-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-3-return"&gt;3&lt;/a&gt;&lt;/sup&gt; In  &lt;em&gt;History of Lisp&lt;/em&gt;,&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-4-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-4-return"&gt;4&lt;/a&gt;&lt;/sup&gt; McCarthy writes that he borrowed the λ-notation from  Alonzo Church&amp;rsquo;s lambda calculus, but none of the other ideas. He also  recounts an incident where a programmer desired lexical scoping, but Lisp  used dynamic scoping. McCarthy considered this to be a bug, which Steve  Russell later fixed by developing the &amp;ldquo;FUNARG device.&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;1963&lt;/strong&gt;. After a few years of work, the &lt;em&gt;Revised Report on Algorithm  Language ALGOL 60&lt;/em&gt; is published.&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-5-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-5-return"&gt;5&lt;/a&gt;&lt;/sup&gt; While &amp;ldquo;lexical scope&amp;rdquo; is not explicitly  mentioned, it is recognizable in the specification.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;1964&lt;/strong&gt;. Peter Landin shows how expressions in programming languages can  be modelled in Church&amp;rsquo;s λ-notation.&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-6-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-6-return"&gt;6&lt;/a&gt;&lt;/sup&gt; He also introduces the concept of a  &lt;em&gt;closure&lt;/em&gt;, which pairs a lambda expression with the environment it was  evaluated in.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;1970&lt;/strong&gt;. Joel Moses describes the problem of free variables in  functions.&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-7-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-7-return"&gt;7&lt;/a&gt;&lt;/sup&gt; He considers both the &amp;ldquo;downward&amp;rdquo; case (where a function is  passed to another function) and the &amp;ldquo;upward&amp;rdquo; case (where a function returns  a function), and remarks on the correspondence between Lisp&amp;rsquo;s FUNARG device  and Landin&amp;rsquo;s closures.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;1975&lt;/strong&gt;. Gerald Sussman and Guy Steele publish the first Scheme paper.&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-8-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-8-return"&gt;8&lt;/a&gt;&lt;/sup&gt;  They describe their goal of a Lisp-like language that is based on the  lambda calculus. As a consequence, they implement lexical scoping with  closures, to preserve the substitution semantics of the lambda calculus.  They compare this scoping discipline to ALGOL&amp;rsquo;s.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;1978&lt;/strong&gt;. Steele and Sussman describe various programming language design  choices, by developing an interpreter for each programming language  variation.&lt;sup&gt;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-9-definition" name="2019-09-05-lexical-and-dynamic-scope-footnote-9-return"&gt;9&lt;/a&gt;&lt;/sup&gt; In particular, they provide a detailed discussion on  lexical and dynamic scoping.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;h2 id="next-stop-r"&gt;Next stop, R&lt;/h2&gt;

&lt;p&gt;Now that we have examined the definitions of lexical and dynamic scope, and also explored some history, we are ready to return to the original question. &lt;em&gt;Is R lexically or dynamically scoped?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the next blog post, we&amp;rsquo;ll answer that question, and also see how R can be very confusing.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I would like to thank Sam Caldwell, Ben Greenman, and Artem Pelenitsyn for their comments and feedback on this blog post.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class="footnotes"&gt;
 &lt;ol&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-1-definition" class="footnote-definition"&gt;
   &lt;p&gt;For example, at one point I defined lexical/dynamic scoping in terms of a  &amp;ldquo;lexical environment&amp;rdquo; and a &amp;ldquo;dynamic environment.&amp;rdquo; But (1) that&amp;rsquo;s a circular  definition, (2) it assumes the reader has some intuition of how a &amp;ldquo;lexical  environment&amp;rdquo; is different from a &amp;ldquo;dynamic environment,&amp;rdquo; and (3) it conflates  two different kinds of &amp;ldquo;environment.&amp;rdquo;&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-1-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-2-definition" class="footnote-definition"&gt;
   &lt;p&gt;G. Steele. &amp;ldquo;Scope and Extent,&amp;rdquo; in &lt;em&gt;Common Lisp the Language&lt;/em&gt;, 2nd ed.  1990. [&lt;a href="https://www.cs.cmu.edu/Groups/AI/html/cltl/clm/node43.html#SECTION00700000000000000000"&gt;Available online&lt;/a&gt;]&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-2-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-3-definition" class="footnote-definition"&gt;
   &lt;p&gt;J. McCarthy. &amp;ldquo;Recursive Functions of Symbolic Expressions and Their  Computation by Machine, Part I,&amp;rdquo; &lt;em&gt;Communications of the ACM&lt;/em&gt;, vol. 3, no. 4,  April 1960. [&lt;a href="https://doi.org/10.1145/367177.367199"&gt;DOI&lt;/a&gt;][&lt;a href="http://jmc.stanford.edu/articles/recursive/recursive.pdf"&gt;Available  online&lt;/a&gt;]&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-3-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-4-definition" class="footnote-definition"&gt;
   &lt;p&gt;J. McCarthy. &amp;ldquo;History of LISP,&amp;rdquo; in &lt;em&gt;History of Programming Languages&lt;/em&gt;,  1978. [&lt;a href="https://doi.org/10.1145/800025.1198360"&gt;DOI&lt;/a&gt;][&lt;a href="http://jmc.stanford.edu/articles/lisp/lisp.pdf"&gt;Available  online&lt;/a&gt;]&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-4-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-5-definition" class="footnote-definition"&gt;
   &lt;p&gt;P. Naur (ed.). &amp;ldquo;Revised Report on Algorithmic Language ALGOL 60,&amp;rdquo;  &lt;em&gt;Communications of the ACM&lt;/em&gt;, vol. 6, no. 1, January 1963.  [&lt;a href="http://dx.doi.org/10.1145/366193.366201"&gt;DOI&lt;/a&gt;][&lt;a href="https://www.masswerk.at/algol60/report.htm"&gt;Available  online&lt;/a&gt;]&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-5-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-6-definition" class="footnote-definition"&gt;
   &lt;p&gt;P. Landin. &amp;ldquo;The mechanical evaluation of expressions,&amp;rdquo; &lt;em&gt;The Computer  Journal&lt;/em&gt;, vol. 6, no. 4, January 1964.  [&lt;a href="https://doi.org/10.1093/comjnl/6.4.308"&gt;DOI&lt;/a&gt;][&lt;a href="https://www.cs.cmu.edu/~crary/819-f09/Landin64.pdf"&gt;Available  online&lt;/a&gt;]&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-6-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-7-definition" class="footnote-definition"&gt;
   &lt;p&gt;J. Moses. &amp;ldquo;The Function of FUNCTION in LISP or Why the FUNARG Problem  Should be Called the Environment Problem,&amp;rdquo; &lt;em&gt;SIGSAM Bulletin 15&lt;/em&gt;, July 1970.  [&lt;a href="https://doi.org/10.1145/1093410.1093411"&gt;DOI&lt;/a&gt;][&lt;a href="https://dspace.mit.edu/handle/1721.1/5854"&gt;Available  online&lt;/a&gt;]&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-7-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-8-definition" class="footnote-definition"&gt;
   &lt;p&gt;G. Sussman and G. Steele. &amp;ldquo;SCHEME: An Interpreter for Extended Lambda  Calculus.&amp;rdquo; 1975. [&lt;a href="https://dspace.mit.edu/handle/1721.1/5794"&gt;Available  online&lt;/a&gt;]&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-8-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2019-09-05-lexical-and-dynamic-scope-footnote-9-definition" class="footnote-definition"&gt;
   &lt;p&gt;G. Steele and G. Sussman. &amp;ldquo;The Art of the Interpreter or, The Modularity  Complex (Parts Zero, One, and Two).&amp;rdquo; 1978. [&lt;a href="https://dspace.mit.edu/handle/1721.1/6094"&gt;Available  online&lt;/a&gt;]&amp;nbsp;&lt;a href="#2019-09-05-lexical-and-dynamic-scope-footnote-9-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</description></item>
  <item>
   <title>On-Stack Replacement</title>
   <link>http://prl.ccs.neu.edu/blog/2019/01/28/on-stack-replacement/?utm_source=by-Ming-Ho-Yee&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2019-01-28-on-stack-replacement</guid>
   <pubDate>Mon, 28 Jan 2019 10:29:57 UT</pubDate>
   <author>PRL</author>
   <description>
&lt;p&gt;Last semester, I took &lt;a href="https://course.ccs.neu.edu/cs7600/"&gt;a course&lt;/a&gt; where the final project was to write a survey paper on &amp;ldquo;a topic in the intersection between computer systems and your area.&amp;rdquo; So I wrote about on-stack replacement.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;On-stack replacement (OSR) is a programming language implementation technique that allows a running program to switch to a different version of code. For example, a program could start executing optimized code, and then transfer to and start executing unoptimized code. This was the original use case for OSR, to facilitate debugging of optimized code.&lt;/p&gt;
 &lt;p&gt;After its original use was established, OSR shifted to a different use case: optimizing programs. OSR allows the run-time system to detect if a program is executing an inefficient loop, recompile and optimize the method that contains the loop, and then transfer control to the newly compiled method. Another strategy is to optimize code based on some assumptions, then, if the assumptions are invalidated at run-time, transfer control back to the original, unoptimized code.&lt;/p&gt;
 &lt;p&gt;In this survey paper, we study how OSR was first introduced as a means for debugging, how it came to be used for program optimizations, its implementation as a reusable library, and other directions of research.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you&amp;rsquo;re interested, you can find a copy &lt;a href="/img/cs7600-mhyee-survey-paper-osr.pdf"&gt;here&lt;/a&gt; or on &lt;a href="https://www.overleaf.com/read/smcmsnksxfdk"&gt;Overleaf&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;If you liked this post, you may also be interested in &lt;a href="http://prl.ccs.neu.edu/blog/2017/03/15/tracing-jits-for-dynamic-languages/"&gt;tracing JITs for dynamic languages&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</description></item>
  <item>
   <title>Spring 2017 PL Junior Retrospective</title>
   <link>http://prl.ccs.neu.edu/blog/2017/06/16/spring-2017-pl-junior-retrospective/?utm_source=by-Ming-Ho-Yee&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-06-16-spring-2017-pl-junior-retrospective</guid>
   <pubDate>Fri, 16 Jun 2017 11:38:25 UT</pubDate>
   <author>PRL</author>
   <description>
&lt;p&gt;The &lt;a href="http://prl.ccs.neu.edu/seminars.html"&gt;PL Junior Seminar&lt;/a&gt; is for beginning PhD and interested undergrad and masters students to understand the foundations of programming languages research. It serves to fill in background knowledge and get up to speed with different areas of PL research.&lt;/p&gt;

&lt;p&gt;For the spring 2017 instance of PL Junior we chose program synthesis, the sequent calculus, and logic programming as topics we wanted to learn more about. We also did two group paper readings for Luca Cardelli&amp;rsquo;s &lt;a href="http://www.lucacardelli.name/Papers/TypefulProg.pdf"&gt;Typeful Programming&lt;/a&gt; and Alan Kay&amp;rsquo;s &lt;a href="http://worrydream.com/EarlyHistoryOfSmalltalk/"&gt;Early History of Smalltalk&lt;/a&gt;. At the same time, we changed up the format from the previous semester.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="format"&gt;Format&lt;/h2&gt;

&lt;p&gt;As discussed in &lt;a href="http://prl.ccs.neu.edu/blog/2017/01/02/fall-2016-pl-junior-retrospective/"&gt;last fall&amp;rsquo;s retrospective&lt;/a&gt;, we wanted to move from group reading and discussion towards weekly presentations. Reading a paper to prepare a presentation is quite a different experience compared to the effort that goes in when it is just for a group discussion (in our experience). With any luck, the presentation will convey some of this deeper knowledge to the rest of the group, with the result being a deep understanding on the part of the presenter and an informed, if somewhat shallower, understanding in the rest of the group. Ideally, the end result should compare favorably to simply reading the paper individually.&lt;/p&gt;

&lt;p&gt;One idea from last semester that we decided to keep is to spend a length of time (possibly an entire semester) on a topic rather than having a new topic each week. Staying on the same theme helps with retention as well as allowing for deeper investigation.&lt;/p&gt;

&lt;p&gt;In that spirit, we chose three themes for the semester: program synthesis, the sequent calculus, and logic programming. Mostly by chance, these topics have interesting connections to each other, and we even had several PL Grown-Up Seminars this semester on program synthesis!&lt;/p&gt;

&lt;h2 id="synthesis"&gt;Synthesis&lt;/h2&gt;

&lt;p&gt;The first paper on program synthesis that we looked at was &lt;a href="https://www.sri.com/sites/default/files/uploads/publications/pdf/725.pdf"&gt;A Deductive Approach to Program Synthesis&lt;/a&gt; by Manna and Waldinger. We chose this paper because it&amp;rsquo;s old and has a lot of citations so it&amp;rsquo;s probably Important. It was interesting and provided an OK introduction to proof search but the method presented seems far removed from modern synthesis techniques.&lt;/p&gt;

&lt;p&gt;The next paper was &lt;a href="https://arxiv.org/abs/1507.02988"&gt;Programmatic and Direct Manipulation, Together&lt;/a&gt; by Chugh, Hempel,  Spradlin, and Alders, which presents the &lt;a href="https://ravichugh.github.io/sketch-n-sketch/index.html"&gt;Sketch-n-Sketch&lt;/a&gt; system. Sketch-n-Sketch is a cool system. It demonstrates that a narrow application of synthesis - trying to fill in the constant values in a program (sketching) - can be used for great effect. We were left wondering, however, if it was too narrow an application of synthesis to give much of an indication of what the entire field is like.&lt;/p&gt;

&lt;p&gt;We concluded our program synthesis segment with &lt;a href="http://www.cis.upenn.edu/~stevez/papers/OZ15.pdf"&gt;Type-and-Example-Directed Program Synthesis&lt;/a&gt; by Osera and Zdancewic, another relatively recent paper. This seems like a relevant paper because we are under the impression that using examples to do synthesis is a big thing right now. Using types to constrain the search is another interesting perspective on techniques for synthesis.&lt;/p&gt;

&lt;p&gt;While each of theses papers had merits, none was so comprehensive as to be a necessary inclusion in any future look at program synthesis for pl junior&lt;/p&gt;

&lt;h2 id="sequent-calculus"&gt;Sequent Calculus&lt;/h2&gt;

&lt;p&gt;We followed up the program synthesis unit with a week on the sequent calculus. The seminar presentation was based on a paper by &lt;a href="https://hal.inria.fr/inria-00381525/document"&gt;Herbelin&lt;/a&gt;. &lt;a href="http://www.ccs.neu.edu/home/gasche/phd_thesis/scherer-thesis.pdf"&gt;Gabriel’s thesis&lt;/a&gt; (chapter 4) includes maybe a more suitable modern introduction to the sequent calculus.&lt;/p&gt;

&lt;p&gt;It might have been better to do sequent calculus first because there is a modern branch of proof search based on the sequent calculus. Presenting this first would have allowed us to look into proof search for program synthesis.&lt;/p&gt;

&lt;p&gt;An additional problem is that it was insufficiently motivated. Either skipping the topic or spending more time on it would be preferable, since one week was just enough to describe the sequent calculus but not enough to apply it. For this topic to be worthwhile, it would best be used as the basis for subsequent readings that directly reference it.&lt;/p&gt;

&lt;h2 id="logic-programming"&gt;Logic Programming&lt;/h2&gt;

&lt;p&gt;The topic was presented over two weeks. The first session presented/demoed Prolog as a language, and we got a sense of what logic programming could do. But it was a whirlwind tour, and we were left wondering about specific details (how proof search runs, what &lt;code&gt;cut&lt;/code&gt; does).&lt;/p&gt;

&lt;p&gt;The second session presented the paper &lt;a href="http://www.doc.ic.ac.uk/~rak/papers/kowalski-van_emden.pdf"&gt;The Semantics of Predicate Logic as a Programming Language&lt;/a&gt;. It was interesting and insightful but left wondering how it relates to the implementation of real logic programming languages.&lt;/p&gt;

&lt;p&gt;In hindsight this was about as far as we could have gotten in just two weeks. However, complications such as the cut rule seem prevalent enough in practice that more time would be required to build up a useful understanding of logic programming&lt;/p&gt;

&lt;h2 id="bonus-rounds"&gt;Bonus Rounds&lt;/h2&gt;

&lt;p&gt;We also used a few weeks to read and discuss specific papers as a group.&lt;/p&gt;

&lt;p&gt;The first paper we read was Cardelli&amp;rsquo;s &lt;a href="http://www.lucacardelli.name/Papers/TypefulProg.pdf"&gt;Typeful Programming&lt;/a&gt;. We picked typeful programming because Matthias has mentioned on occasion how important he thinks it is.&lt;/p&gt;

&lt;p&gt;It was an interesting read; more of an essay than a paper. It really stood out as different from the other academic publications that we have looked at. It’s a walk through of a language design motivating each design decision in practical terms, as in things that actually help the programmer.&lt;/p&gt;

&lt;p&gt;Cardelli places great importance on polymorphism (subtyping in addition to parametric), as well as features for programming in the large such as modules and interfaces. Several features are interesting in their omission, like type inference and macros.&lt;/p&gt;

&lt;p&gt;After reading it it’s not clear why Matthias thinks it’s so important. From the perspective of modern researchers, many of the features in Cardelli&amp;rsquo;s language seem rather mundane. However, it&amp;rsquo;s likely that at the time he published it, these ideas were significantly newer and much less widespread.&lt;/p&gt;

&lt;p&gt;The other paper we read as a group was Alan Kay&amp;rsquo;s &lt;a href="http://worrydream.com/EarlyHistoryOfSmalltalk/"&gt;The Early History of Smalltalk&lt;/a&gt;. It seems like the Smalltalk project investigated a plethora of interesting ideas about designing programming languages and environments. This article seems to confirm that but does not delve into many particulars.&lt;/p&gt;

&lt;h2 id="final-thoughts"&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;Overall this semester of pl junior went well enough that we think it makes a reasonable template for future semesters. The topics were interesting and relevant, and we mostly picked appropriate material for presentations. One downside is that we didn’t quite ‘fill out’ the semester with presentations due to scheduling and not wanting to make some people present twice. Here’s a lesson: recruit more people to the phd program (or get more undergrads) so you don’t have this problem!&lt;/p&gt;

&lt;p&gt;Having papers in a theme helped a lot over previous paper-presentation iterations of pl junior. It helped each week being able to build on what we learned last week, as opposed to having a whirlwind of unrelated topics.&lt;/p&gt;

&lt;p&gt;Writing this retrospective has also proven to be a beneficial exercise. Especially with our sequences of connected topics, looking back has allowed us to put the earlier papers into perspective and better assess both their relevance and presentation.&lt;/p&gt;</description></item>
  <item>
   <title>Report: PLISS 2017</title>
   <link>http://prl.ccs.neu.edu/blog/2017/06/05/report-pliss-2017/?utm_source=by-Ming-Ho-Yee&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-06-05-report-pliss-2017</guid>
   <pubDate>Mon, 05 Jun 2017 15:47:59 UT</pubDate>
   <author>PRL</author>
   <description>
&lt;p&gt;Two weeks ago, I attended the first &lt;a href="https://pliss2017.github.io/"&gt;Programming Language Implementation Summer School&lt;/a&gt;, held in beautiful Bertinoro, Italy.&lt;/p&gt;

&lt;p&gt;The goal of PLISS was &amp;ldquo;to prepare early graduate students and advanced undergraduates for research in the field,&amp;rdquo; and I think it successfully accomplished that. There were many talks in a variety of areas, such as just-in-time compilers, garbage collection, static analysis, and distributed systems. But PLISS was more than just a series of talks: PLISS provided an environment for interacting with other students as well as senior researchers.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="the-talks"&gt;The Talks&lt;/h2&gt;

&lt;p&gt;With the amount of technical content at PLISS, there was easily something for everyone. &lt;a href="http://janvitek.org/"&gt;Jan Vitek&lt;/a&gt; and &lt;a href="http://tratt.net/laurie/"&gt;Laurence Tratt&lt;/a&gt; gave lectures that included hands-on exercises where we worked on JITs. &lt;a href="https://www.cs.purdue.edu/homes/suresh/"&gt;Suresh Jagannathan&lt;/a&gt; dived into the operational semantics of a distributed system, so we could reason about different weak consistency models. Francesco Logozzo gave us a whirlwind tour of abstract interpretation.&lt;/p&gt;

&lt;p&gt;Most of my favorite talks included some form of extra content, such as exercises, live-coding presentations, or demos. I found it really helpful to write actual code and apply what I had just learned, or to look at some concrete examples. The examples and exercises also helped with the pacing, as actively listening to four 90-minute talks every day is exhausting!&lt;/p&gt;

&lt;p&gt;Off the top of my head, these were some of my favorite talks:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;Dynamic Programming Language Implementation with LLVM&lt;/strong&gt;, by Petr Maj, Oli  Flückiger, and &lt;a href="http://janvitek.org/"&gt;Jan Vitek&lt;/a&gt;. As the first talk of the summer school, this  was a gentle introduction for the rest of the week. We had &lt;a href="https://github.com/PRL-PRG/pliss-rift/"&gt;exercises&lt;/a&gt;  (with intentional bugs to make us think!), and also brief overviews of  intermediate languages, static analysis, and garbage collection. These three  topics would later show up in more detail.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;Micro Virtual Machines&lt;/strong&gt;, by &lt;a href="http://users.cecs.anu.edu.au/~steveb/"&gt;Steve Blackburn&lt;/a&gt;. This talk covered  background information on virtual machines, and also the &lt;a href="http://microvm.github.io/"&gt;Micro VM&lt;/a&gt;  project that Steve&amp;rsquo;s group has been working on. A lot of the material was  already familiar to me, but I still enjoyed the talk, and even got a few  ideas for the project I&amp;rsquo;m working on!&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;Static Analysis&lt;/strong&gt;, by &lt;a href="http://matt.might.net/"&gt;Matt Might&lt;/a&gt;. Matt&amp;rsquo;s talk was based on one of  his &lt;a href="http://matt.might.net/articles/intro-static-analysis/"&gt;articles&lt;/a&gt; and an older talk he&amp;rsquo;s given. Impressively, the entire  example was live-coded, with only a single mistake!&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;strong&gt;Testing Language Implementations&lt;/strong&gt;, by &lt;a href="http://multicore.doc.ic.ac.uk/"&gt;Alastair Donaldson&lt;/a&gt;. This was  an entertaining talk, since Ally showed multiple examples of crashing  compilers, and causing other kinds of mischief by triggering compiler bugs.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;If you&amp;rsquo;re disappointed that you couldn&amp;rsquo;t see these talks, don&amp;rsquo;t worry! The talks were recorded and will be posted very shortly.&lt;/p&gt;

&lt;h2 id="the-people"&gt;The People&lt;/h2&gt;

&lt;p&gt;But there&amp;rsquo;s more to PLISS than the talks. I&amp;rsquo;m referring to &lt;em&gt;networking&lt;/em&gt;, or the opportunity to get out and talk to other people about research.&lt;/p&gt;

&lt;p&gt;As an early graduate student, I&amp;rsquo;ve been given a lot of advice about talking to people at conferences and the importance of the &amp;ldquo;hallway track.&amp;rdquo; I still have difficulty doing this at an actual conference, like &lt;a href="http://pldi17.sigplan.org/home"&gt;PLDI&lt;/a&gt; or &lt;a href="http://2017.ecoop.org/"&gt;ECOOP&lt;/a&gt;. When there are hundreds of attendees, or when people already know each other and are in conversation groups, I find it difficult to approach them.&lt;/p&gt;

&lt;p&gt;This was not the case at PLISS. There were fewer attendees: about fifty students and a dozen speakers. There was a good mix of undergraduate, master&amp;rsquo;s, first-year PhD, and more senior PhD students. All our breakfasts, lunches, and breaks were together, so we would see the same people again and again, and inevitably start to learn each other&amp;rsquo;s names. The speakers would also be among us, and there was a good ratio of speakers to students for discussions and mealtime mentoring.&lt;/p&gt;

&lt;p&gt;I had many opportunities to practice my &amp;ldquo;research pitch.&amp;rdquo; I talked to senior students and got advice. I talked to junior students and gave advice. Two different people I talked to about my research pointed me to the same paper to read. I found another student who was working with &lt;a href="http://research.cs.wisc.edu/wpis/papers/popl95.pdf"&gt;IFDS&lt;/a&gt;, an algorithm I have spent much time trying to understand. And, one day at lunch, my table discovered that we were all working on static analysis!&lt;/p&gt;

&lt;p&gt;As much as I enjoyed the talks, I think the best part of PLISS was meeting and talking to other people. You can replace talks with videos (but you lose the speaker-audience interaction), and you can replace conversations with other forms of communication. But there isn&amp;rsquo;t really anything that can replace the serendipity of bumping into someone with a shared interest.&lt;/p&gt;

&lt;h2 id="the-location"&gt;The Location&lt;/h2&gt;

&lt;p&gt;Actually, the &lt;em&gt;other&lt;/em&gt; best part of PLISS was the location. Italy is a beautiful country with delicious food. And Bertinoro is a small town on the top of a hill, with a breathtaking view of the surrounding area. The lectures were held in a &lt;a href="https://pliss2017.github.io/images/pics/7.jpg"&gt;castle at the top of the hill&lt;/a&gt; (photo credit: Steve Blackburn). The speakers lived in the castle for the week, while the students lived in the former monastery (seems fitting), which has been renovated into a university residence.&lt;/p&gt;

&lt;p&gt;Here are my two favorite pictures I took (click for full size):&lt;/p&gt;

&lt;p&gt;&lt;a href="/img/pliss2017-1.jpg"&gt;&lt;img src="/img/pliss2017-1-thumb.jpg" alt="View from the castle" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="/img/pliss2017-2.jpg"&gt;&lt;img src="/img/pliss2017-2-thumb.jpg" alt="Panorama" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Steve Blackburn has more pictures posted on the &lt;a href="https://pliss2017.github.io/"&gt;PLISS website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="final-thoughts"&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;PLISS was a wonderful event. Many thanks need to be given to the speakers, organizers, and sponsors, for making this possible!&lt;/p&gt;

&lt;p&gt;If and when there is a second PLISS, I highly encourage students to apply! You will learn a lot from the lectures, from talking to the speakers, and meeting other students. And if it&amp;rsquo;s in Bertinoro again, you can enjoy the weather and nice view!&lt;/p&gt;</description></item>
  <item>
   <title>Tracing JITs for Dynamic Languages</title>
   <link>http://prl.ccs.neu.edu/blog/2017/03/15/tracing-jits-for-dynamic-languages/?utm_source=by-Ming-Ho-Yee&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-03-15-tracing-jits-for-dynamic-languages</guid>
   <pubDate>Wed, 15 Mar 2017 10:54:39 UT</pubDate>
   <author>PRL</author>
   <description>&lt;!-- more--&gt;

&lt;p&gt;Traditional JIT (just-in-time) compilers are method-based: they compile &amp;ldquo;hot&amp;rdquo; (i.e. frequently executed) methods to native code. An alternative is trace-based or tracing JITs, where the compilation unit is a (hot) sequence of instructions. Typically, such sequences of instructions correspond to loops, where programs spend most of their execution time.&lt;/p&gt;

&lt;p&gt;Where did the idea of tracing come from? What was appealing about it? How was tracing adapted for JITs and dynamic languages? What happened to Mozilla&amp;rsquo;s TraceMonkey, which used to be part of Firefox? Do any JITs today use tracing?&lt;/p&gt;

&lt;p&gt;In this talk, I trace tracing JITs from their origins to some of their recent developments. I cover five papers: the original tracing paper, an implementation of a tracing JIT for Java, the TraceMonkey JIT for JavaScript, PyPy&amp;rsquo;s &amp;ldquo;meta-level&amp;rdquo; tracing, and a specific class of optimizations for tracing JITs.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(The idea of using the phrase &amp;ldquo;trace tracing JITs&amp;rdquo; is from Matthias Felleisen.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All materials can be found in the &lt;a href="https://github.com/nuprl/hopl-s2017/tree/master/tracing-jit"&gt;course repository&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/tracing-jit/notes.pdf"&gt;Full notes&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://github.com/nuprl/hopl-s2017/blob/master/tracing-jit/annotated.txt"&gt;Annotated bibliography&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;If you liked this post, you may also be interested in &lt;a href="http://prl.ccs.neu.edu/blog/2019/01/28/on-stack-replacement/"&gt;on-stack replacement&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</description></item>
  <item>
   <title>Fall 2016 PL Junior Retrospective</title>
   <link>http://prl.ccs.neu.edu/blog/2017/01/02/fall-2016-pl-junior-retrospective/?utm_source=by-Ming-Ho-Yee&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-prl-ccs-neu-edu:-blog-2017-01-02-fall-2016-pl-junior-retrospective</guid>
   <pubDate>Mon, 02 Jan 2017 16:39:37 UT</pubDate>
   <author>PRL</author>
   <description>
&lt;p&gt;The &lt;a href="http://prl.ccs.neu.edu/seminars.html"&gt;Programming Language Seminar, Junior&lt;/a&gt; (or “PL Junior”), is a seminar for junior students to learn and discuss topics at a pace more suitable to our background. This semester, we decided to study dependent types. We chose this topic because&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;working from the &lt;a href="https://mitpress.mit.edu/books/types-and-programming-languages"&gt;TAPL&lt;/a&gt; presentation of type systems, dependent types are a step up in difficulty (excepting F-omega-sub), and&lt;/li&gt;
 &lt;li&gt;they represent a significant increase in the reasoning power of types over programs.&lt;/li&gt;&lt;/ol&gt;
&lt;!-- more--&gt;

&lt;p&gt;There was a preference for learning how to implement a dependent type system, instead of spending a significant amount of time reading papers, especially dense type-theoretic papers suggested by &lt;a href="http://purelytheoretical.com/sywtltt.html"&gt;posts&lt;/a&gt; like &lt;a href="http://jozefg.bitbucket.org/posts/2015-08-14-learn-tt.html"&gt;these&lt;/a&gt;. So we followed the &lt;a href="https://github.com/sweirich/pi-forall"&gt;pi-for-all&lt;/a&gt; lecture series given by Stephanie Weirich at &lt;a href="https://www.cis.upenn.edu/~bcpierce/attapl/"&gt;OPLSS&lt;/a&gt;, which focuses on implementing a simple dependently-typed programming language.&lt;/p&gt;

&lt;p&gt;After the pi-for-all lectures, we read chapter two of Edwin Brady’s &lt;a href="https://eb.host.cs.st-andrews.ac.uk/writings/thesis.pdf"&gt;dissertation on implementing dependently typed languages&lt;/a&gt;. The thesis includes a relatively approachable introduction to TT, the core dependent type theory of Epigram.&lt;/p&gt;

&lt;p&gt;Along the way, we became sidetracked by &lt;a href="https://en.wikipedia.org/wiki/System_U#Girard.27s_paradox"&gt;Girard’s paradox&lt;/a&gt;. In the first pi-for-all lecture, we came across the Type-in-Type rule. (In a dependent type system the term and the type languages are the same. However, we still need to distinguish what is a “program” and what is a “type,” for instance, so that we can determine that the annotation of a function’s argument is valid. So a construct in the term language is Type, which is meant to describe those things that are valid in programs where we expect to find a type). In the lecture, this prompted the comment that this (“of course”) makes our system inconsistent as a logic, but there was no further elaboration, and we could not figure out how to use this fact to show inconsistency.&lt;/p&gt;

&lt;p&gt;It turns out the reason Type-in-Type is inconsistent is quite complicated. It is explained in a &lt;a href="https://www.cs.cmu.edu/~kw/scans/hurkens95tlca.pdf"&gt;paper&lt;/a&gt; that we had difficulty understanding. So we turned to the students in our lab that have expertise in the area. The answer we received is that, intuitively, it is inconsistent for the same reason as Russell’s paradox (or the Burali-Forti paradox), but the actual proof is actually quite involved. The lesson we drew is that despite being “obvious,” Type-in-Type being inconsistent is not easy to prove. The way people seem to throw around this conclusion is confusing from a beginner’s point of view.&lt;/p&gt;

&lt;p&gt;The best thing about the pi-for-all series is that it demystified dependent types for us. We gained confidence in being able to whiteboard a dependent type system with the ease of System-F or STLC. If we had one complaint, the presentation of the material relied heavily on Haskell details. The unbound library to handle variables in the implementation results in a somewhat “magicy” representation of binding; it’s not clear that the benefits are so great as to outweigh the cost of just implementing alpha-equivalence and capture-avoiding-substitution. Overall they were high-quality lectures. As hinted above, we didn’t particularly care for the second lecture that was mostly a code walk-through. One advantage of watching videos was that we could speed through parts we were already comfortable with.&lt;/p&gt;

&lt;p&gt;With Edwin Brady’s dissertation, we got a glimpse of how quickly the details of a dependently typed language get hairy. Looking at you, inductive data definitions and eliminations. There were some extremely large type signatures. While this exercise boosted our confidence that we could read Serious Dependent Types™ papers, it also gave evidence that our fears of incomprehensibility were not completely unfounded.&lt;/p&gt;

&lt;p&gt;This issue appeared before in our discussion of Girard’s Paradox. In the discussion of the paradox, we got stuck when we tried to understand the very complex term that inhabited the bottom type. Dependent typing, and discussions thereof, allow very rich, meaningful, and complex types that are as complex as the code that they abstract over. While we are used to understanding these structures in code, parsing a complex type and its fundamental meaning gave us considerable difficulty.&lt;/p&gt;

&lt;h2 id="thoughts-on-the-format"&gt;Thoughts on the format&lt;/h2&gt;

&lt;p&gt;Our meetings this semester were all of the form “we’ll watch this lecture or read this chapter, and then discuss it next week.” Next semester we would like to go back to presenting each week. We feel doing presentations forces the presenter to reach a deeper understanding of the material. This semester we got a relatively shallow understanding of a broad area. A deeper understanding with a narrower focus may be more beneficial (or complementary).&lt;/p&gt;

&lt;p&gt;[[Sam’s defense as Grand Convener of PL Junior: Once we picked dependent types as a topic, doing presentations was not an option. We didn’t have the expertise in the area to pick out different sub-topics and papers suitable for presentations. And, since we were short-handed (4 people each week), we would be presenting once a month!]]&lt;/p&gt;

&lt;p&gt;If we continue learning more about dependent types it would be by: 1) Doing more reading, such as the &lt;a href="https://www.cis.upenn.edu/~bcpierce/attapl/"&gt;ATTAPL&lt;/a&gt; chapter or the programming in Martin-Löf’s type theory material. 2) Actually trying to implement some of the things we’ve learned this semester 3) Playing around with more of the various dependent type systems and theorem provers out there&lt;/p&gt;

&lt;p&gt;For future pl junior cohorts or anyone else in learning about dependent types: Pi-for-all is useful material, but could be condensed into two weeks (for example, by watching the lectures at 1.5x speed) instead of four. Don’t worry about Type-in-Type. The Epigram material is OK but you might be better served looking at ATTAPL first. At some point, you will have to read the dense papers, but pi-for-all is a good introduction.&lt;/p&gt;</description></item></channel></rss>